{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, InputLayer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import optimizers\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "X = []\n",
    "for filename in os.listdir('../Full-version/Train/'):\n",
    "    X.append(img_to_array(load_img('../Full-version/Train/'+filename)))\n",
    "X = np.array(X, dtype=float)\n",
    "\n",
    "# Set up train and test data\n",
    "split = int(0.95*len(X))\n",
    "Xtrain = X[:split]\n",
    "Xtrain = 1.0/255*Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_206 (Conv2D)          (None, 256, 256, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_207 (Conv2D)          (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_208 (Conv2D)          (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_209 (Conv2D)          (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_210 (Conv2D)          (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_211 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_212 (Conv2D)          (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_213 (Conv2D)          (None, 32, 32, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_214 (Conv2D)          (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_50 (UpSampling (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_215 (Conv2D)          (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_51 (UpSampling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_216 (Conv2D)          (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_217 (Conv2D)          (None, 128, 128, 2)       578       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_52 (UpSampling (None, 256, 256, 2)       0         \n",
      "=================================================================\n",
      "Total params: 3,892,194\n",
      "Trainable params: 3,892,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(256, 256, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "#model.compile(optimizer='rmsprop', loss='mse',metrics=['acc'])\n",
    "#model.compile(optimizer=optimizers.RMSprop(lr=1e-4), loss='mse',metrics=['acc'])\n",
    "#model.compile(optimizer=optimizers.Adam(lr=1e-4), loss='mse',metrics=['acc'])\n",
    "\n",
    "#optz = optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#optz = optimizers.RMSprop(lr=0.00001)\n",
    "optz = optimizers.Adam(lr=0.00001)\n",
    "#optz = 'categorical_crossentropy'\n",
    "#optz='rmsprop'\n",
    "model.compile(optimizer=optz, loss='mse',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 70s 350ms/step - loss: 0.0102 - acc: 0.5480\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0100 - acc: 0.5519\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0100 - acc: 0.5542\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0099 - acc: 0.5591\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.0100 - acc: 0.5545\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.0100 - acc: 0.5565\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.0094 - acc: 0.5594\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.0102 - acc: 0.5620\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.0100 - acc: 0.5583\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5546\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0099 - acc: 0.5577\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5615\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0102 - acc: 0.5623\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5578\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5645\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5636\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0100 - acc: 0.5653\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5597\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0097 - acc: 0.5692\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5638\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5633\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0099 - acc: 0.5639\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5674\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5666\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5735\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0101 - acc: 0.5653\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5673\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5749\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5579\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5672\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5680\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0100 - acc: 0.5705\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5762\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5681\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0100 - acc: 0.5730\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5740\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5707\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5692\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5726\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5673\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0098 - acc: 0.5721\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5706\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5683\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5797\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0091 - acc: 0.5704\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5724\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5684\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5769\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5746\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5704\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5723\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5724\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5725\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5740\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5749\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5762\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5725\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5676\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5748\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5728\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5784\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5760\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0090 - acc: 0.5813\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5741\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0101 - acc: 0.5727\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0088 - acc: 0.5742\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5800\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0090 - acc: 0.5731\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5806\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5783\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0091 - acc: 0.5708\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0091 - acc: 0.5791\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0097 - acc: 0.5711\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5787\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5788\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5778\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5757\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5822\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5790\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0096 - acc: 0.5787\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5782\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0091 - acc: 0.5835\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5850\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5719\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5811\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0094 - acc: 0.5768\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5820\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5835\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5846\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0089 - acc: 0.5758\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5826\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0089 - acc: 0.5873\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5809\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5794\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5830\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5852\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0090 - acc: 0.5793\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0093 - acc: 0.5815\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0095 - acc: 0.5805\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0092 - acc: 0.5876\n"
     ]
    }
   ],
   "source": [
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Generate training data\n",
    "batch_size = 5\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n",
    "\n",
    "# Train model      \n",
    "tensorboard = TensorBoard(log_dir=\"output/first_run\")\n",
    "history = model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=100, steps_per_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 25ms/step\n",
      "[0.014036951493471861, 0.5152743458747864]\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "Xtest = rgb2lab(1.0/255*X[split:])[:,:,:,0]\n",
    "Xtest = Xtest.reshape(Xtest.shape+(1,))\n",
    "Ytest = rgb2lab(1.0/255*X[split:])[:,:,:,1:]\n",
    "Ytest = Ytest / 128\n",
    "print(model.evaluate(Xtest, Ytest, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('../Full-version/Test/'):\n",
    "    color_me.append(img_to_array(load_img('../Full-version/Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "# Test model\n",
    "output = model.predict(color_me)\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
