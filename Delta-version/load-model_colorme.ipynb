{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n",
      "keras version 2.1.4\n",
      "tensorflow version 1.5.0\n",
      "numpy version 1.14.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import optimizers\n",
    "from keras.models import model_from_json\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "print(\"Python version\",sys.version)\n",
    "print (\"keras version\", keras.__version__)\n",
    "print (\"tensorflow version\", tf.__version__)\n",
    "print (\"numpy version\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "X = []\n",
    "for filename in os.listdir('data/images/Train/'):\n",
    "    X.append(img_to_array(load_img('data/images/Train/'+filename)))\n",
    "X = np.array(X, dtype=float)\n",
    "Xtrain = 1.0/255*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights\n",
    "#inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "#inception.load_weights('inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 128, 128, 64) 640         input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 128, 128, 128 73856       conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 64, 64, 256)  295168      conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 32, 32, 512)  1180160     conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 32, 32, 512)  2359808     conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 1024, 1000)   0           input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 32, 32, 256)  1179904     conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 32, 32, 1000) 0           repeat_vector_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 1256) 0           conv2d_330[0][0]                 \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 32, 32, 256)  321792      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 32, 32, 128)  295040      conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling2D) (None, 64, 64, 128)  0           conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 64, 64, 64)   73792       up_sampling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling2D) (None, 128, 128, 64) 0           conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 128, 128, 32) 18464       up_sampling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 128, 128, 16) 4624        conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128, 128, 16) 0           conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 128, 128, 2)  290         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling2D) (None, 256, 256, 2)  0           conv2d_336[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,541,202\n",
      "Trainable params: 6,541,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loaded weights from checkpoint\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_colorme.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "optz = optimizers.Adam(lr=0.00001)\n",
    "#optz = optimizers.rmsprop(lr=0.00001)\n",
    "model.compile(optimizer=optz, loss='mse',metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "# load weights into new model\n",
    "#model.load_weights(\"model.h5\")\n",
    "model.load_weights(\"colorme_weight10.h5\",by_name=True)\n",
    "print(\"Loaded weights from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1,save_best_only=True,mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embedding\n",
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.4,\n",
    "        zoom_range=0.4,\n",
    "        rotation_range=40,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "#Generate training data\n",
    "batch_size = 5\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "c:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\color\\colorconv.py:985: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "model.fit_generator(image_a_b_gen(batch_size), epochs=100, steps_per_epoch=200, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model_colorme.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"colorme_weight11.h5\")\n",
    "\n",
    "#Make predictions on validation images\n",
    "color_me = []\n",
    "for filename in os.listdir('data/images/Test/'):\n",
    "    color_me.append(img_to_array(load_img('data/images/Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
